{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python para Economistas: Septima Clase\n",
    "\n",
    "### El problema de predicción:\n",
    "\n",
    "Usualmente los investigadores y analistas buscan construir herramientas que permitan predecir que es lo que va a ocurrir (por ejemplo si una persona será admitida a algun programa, o si un estudiante desertará de la escuela, entre otros) en base a características (a las cuales llamaremos variables explicativas). Esta predicción permite al usuario o al proveedor planificar y tomar decisiones hoy, anticipandose (con cierto grado de confianza) a lo que pueda ocurrir en el futuro.\n",
    "\n",
    "<img src=\"decision_making_flow.png\" alt=\"Drawing\" style=\"width: 600px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffalcon\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# Problema de prediccion binaria:\n",
    "\n",
    "# Definimos o cargamos base de datos:\n",
    "\n",
    "candidates = {'Puntaje Examen': [780,750,690,710,680,730,690,720,740,690,610,690,710,680,770,610,580,650,540,590,620,600,550,550,570,670,660,580,650,660,640,620,660,660,680,650,670,580,590,690],\n",
    "              'GPA': [4,3.9,3.3,3.7,3.9,3.7,2.3,3.3,3.3,1.7,2.7,3.7,3.7,3.3,3.3,3,2.7,3.7,2.7,2.3,3.3,2,2.3,2.7,3,3.3,3.7,2.3,3.7,3.3,3,2.7,4,3.3,3.3,2.3,2.7,3.3,1.7,3.7],\n",
    "              'Experiencia Laboral': [3,4,3,5,4,6,1,4,5,1,3,5,6,4,3,1,4,6,2,3,2,1,4,1,2,6,4,2,6,5,1,2,4,6,5,1,2,1,4,5],\n",
    "              'Admitido': [1,1,0,1,0,1,0,1,1,0,0,1,1,0,1,0,0,1,0,0,1,0,0,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,0,1]\n",
    "              } # Fuente \"https://datatofish.com/logistic-regression-python/\"\n",
    "\n",
    "df = pd.DataFrame(candidates)\n",
    "\n",
    "X = df[['Puntaje Examen', 'GPA', 'Experiencia Laboral']]\n",
    "Y = df[['Admitido']]\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, Y) # Definir el problema\n",
    "clf.coef_\n",
    "clf.predict(X) #Predecir el modelo\n",
    "Y_score = clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold our probabilities\n",
    "Y=np.array(Y).reshape(Y.shape[0])\n",
    "Y_hat=np.array(Y_score[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our Thresholded prob to boolean\n",
    "Y = Y.astype(bool)\n",
    "Y_hat = (Y_hat>.5).astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfusionMatrix(Y, Y_hat):\n",
    "\n",
    "    TP = Y[Y_hat == True].sum()\n",
    "    FP = (~Y[Y_hat == True]).sum()\n",
    "    FN = Y[Y_hat == False].sum()\n",
    "    TN = (~Y[Y_hat == False]).sum()\n",
    "\n",
    "    confusionMatrix = np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "    return confusionMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get confusion matrix using our function\n",
    "myConfusionMatrix = getConfusionMatrix(Y, Y_hat)\n",
    "\n",
    "#Get confusion matrix using scickitlearn function\n",
    "sklearnConfusionMatrix = confusion_matrix(Y, Y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy: % de resultados que el modelo predijo correctamente.\n",
    "clf.score(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision: Cuantos items seleccionados son relevantes\n",
    "def getPrecision(Y, Y_hat):\n",
    "\n",
    "    TP = Y[Y_hat == True].sum()\n",
    "    FP = (~Y[Y_hat == True]).sum()\n",
    "    precision = TP/(TP+FP)\n",
    "\n",
    "    return precision\n",
    "\n",
    "#Recall: Cuantos items relevantes son seleccionados\n",
    "def getRecall(Y, Y_hat):\n",
    "\n",
    "    TP = Y[Y_hat == True].sum()\n",
    "    FN = Y[Y_hat == False].sum()\n",
    "    recall = TP/(TP+FN)\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "# Usamos las funciones basados en un thresholding de .5:\n",
    "\n",
    "getPrecision(Y,Y_hat)\n",
    "getRecall(Y,Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholdingPrecisionRecall(Y, Y_score):\n",
    "\n",
    "    thresholds = np.unique(Y_score)\n",
    "    precisionList = []\n",
    "    recallList = []\n",
    "\n",
    "    for tt in thresholds:\n",
    "\n",
    "        Y_hat = Y_score > tt\n",
    "        precision = getPrecision(Y,Y_hat)\n",
    "        recall = getRecall(Y,Y_hat)\n",
    "        precisionList.append(precision)\n",
    "        recallList.append(recall)\n",
    "\n",
    "    return precisionList, recallList\n",
    "\n",
    "\n",
    "precisionList, recallList = thresholdingPrecisionRecall(Y, Y_score[:,1])\n",
    "\n",
    "\n",
    "# Plot Results:\n",
    "plt.plot(recallList, precisionList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(LogisticRegression(random_state=0).fit(X,Y), X, Y)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTruePositiveRate(Y, Y_hat):\n",
    "\n",
    "    TP = Y[Y_hat == True].sum()\n",
    "    FP = (~Y[Y_hat == True]).sum()\n",
    "    FN = Y[Y_hat == False].sum()\n",
    "    TN = (~Y[Y_hat == False]).sum()\n",
    "\n",
    "    tpr = TP/(TP+FN)\n",
    "\n",
    "    return tpr\n",
    "\n",
    "\n",
    "def getFalsePositiveRate(Y, Y_hat):\n",
    "\n",
    "    TP = Y[Y_hat == True].sum()\n",
    "    FP = (~Y[Y_hat == True]).sum()\n",
    "    FN = Y[Y_hat == False].sum()\n",
    "    TN = (~Y[Y_hat == False]).sum()\n",
    "\n",
    "    fpr = FP/(FP+TN)\n",
    "\n",
    "    return fpr\n",
    "\n",
    "def thresholdingAUC(Y, Y_score):\n",
    "\n",
    "    thresholds = np.unique(Y_score)\n",
    "    tprList = []\n",
    "    fprList = []\n",
    "\n",
    "    for tt in thresholds:\n",
    "\n",
    "        Y_hat = Y_score > tt\n",
    "        tpr = getTruePositiveRate(Y,Y_hat)\n",
    "        fpr = getFalsePositiveRate(Y,Y_hat)\n",
    "        tprList.append(tpr)\n",
    "        fprList.append(fpr)\n",
    "\n",
    "    return tprList, fprList\n",
    "\n",
    "\n",
    "tprList, fprList = thresholdingAUC(Y, Y_score[:,1])\n",
    "AUC = metrics.auc(fprList, tprList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the figure:\n",
    "plt.plot(fprList, tprList, label='ROC curve (area = %0.2f)' % AUC)\n",
    "plt.plot(fprList,fprList, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
